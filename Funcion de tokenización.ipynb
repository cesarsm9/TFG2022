{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCIÓN DE TOKENIZACIÓN DE TEXTOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para hacer el análisis de texgtos es necesario tener cada palabra como registro, por ello se pasa a realziar un algoritmo que genera una tabla multi indice en donde cada registro contiene una palabra que está ligado a su texto y a su universidad. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSEUDOCÓDIGO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entender mejor el procedimiento del código, se realizará un pseudocódigo más cercano al lenguaje humano.\n",
    "\n",
    "Concetar a la base de datos<br>\n",
    "Recoger columnas necesarias de la tabla<br>\n",
    "Selecionar los textos<br>\n",
    "hacer todos los caracteres minusculas<br>\n",
    "tokenizar mediante algoritmo<br>\n",
    "para cada palabra<br>\n",
    "&emsp; identificar si es una stopword<br>\n",
    "&emsp; si es una stopword <br>\n",
    "&emsp; &emsp; clasificaren True<br>\n",
    "&emsp; si no lo es<br>\n",
    "&emsp; &emsp; clasificar en false<br>\n",
    "\n",
    "&emsp; crear tabla<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIBRERÍAS\n",
    "\n",
    "\n",
    "**__NLTK__**: Una de las librerías de Python más populares que se asocia a las palabrqas Natural Language Toolkit, que en su traducción al español significa algo así como la mochila de herramientas para el lenguaje natural. Está librería se encarga de que las máquinas entiendan el lenguaje humano y se provee a la librería con infinidad de herramientas para codificar programas para el procesamiento del lenguaje natural. De está librería se usará el TweetTokenizer, que nos permite individualizar cada palabra, signo o elemento y registrarlo en una fila individual creando un multiindex. También se ha usado el corpus para descargar una lista de stopwords en español.\n",
    ", se puede consultar la documentación  __[aquí](https://www.nltk.org/)__.<br><br>\n",
    "\n",
    "**SQL_ Alchemy**: Esta librería ayuda a generar una tabla de datos dentro de la base de datos seleccionada, la peculiaridad es que introduce los datos desde un dataframe de pandas en un proceso muy ágil, se puede consultar la documentación  __[aquí](https://www.sqlalchemy.org/)__.<br><br>\n",
    "\n",
    "Otras librerías que se usan son:\n",
    "- **Pandas** : Manipulación de datos\n",
    "- **Mysql.connector**: Conexión con la base de datos\n",
    "- **Codecs**: Normalización de caracteres\n",
    "- **tqdm**: Enseña barra de progreso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez ya se explica todo, se da comienzo con el código de la tokenización, importando las librerias y creando la conexión a la base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import codecs\n",
    "import pandas as pd\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "codecs.register(lambda name: codecs.lookup('utf8') if name == 'utf8mb4' else None)\n",
    "\n",
    "database = mysql.connector.connect(\n",
    "    host= \"localhost\",\n",
    "    user= \"root\",\n",
    "    passwd= \"\",\n",
    "    database= \"TFG\",\n",
    "    use_unicode = True, \n",
    "    charset=\"utf8mb4\",\n",
    "    collation = \"utf8mb4_general_ci\"\n",
    ")\n",
    "database.set_charset_collation(\"utf8mb4\",\"utf8mb4_general_ci\")\n",
    "\n",
    "cursor= database.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "APrimero, se crean dos funciones que serán útiles luego en el código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minusculas(x): #convierte el texto a minusculas\n",
    "    try:\n",
    "       x =  x.lower()\n",
    "    except:\n",
    "        pass\n",
    "    return x\n",
    "def tokeni(x):# tokeniza los textos\n",
    "    try:\n",
    "        x = x.split(x)\n",
    "    except:\n",
    "        pass\n",
    "    return  x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acto seguido, se ejecuta el código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql(\"SELECT * FROM perfiles_linkedin\",database)\n",
    "\n",
    "df[\"Especialidades\"] = df[\"Especialidades\"].apply(lambda x: minusculas(x))\n",
    "df[\"tokens_espanol\"] = df[\"Especialidades\"].apply(lambda x: tokeni(x))\n",
    "\n",
    "orden = df.explode(column= \"tokens_espanol\")\n",
    "palabrasstop = list(stopwords.words(\"spanish\"))\n",
    "palabrasstop.extend(list(string.punctuation))\n",
    "palabrasstop.extend([\"  \", \" \", \"\\n\",\"¡\",\"¿\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se seleccionan los textos y se crea la lista de palabras stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sino = []\n",
    "for palabra in tqdm(orden[\"tokens_espanol\"]):#para cada palaabra\n",
    "    if type(palabra) != str:#si no es texto\n",
    "            sino.append(\"True\")\n",
    "    elif palabra in palabrasstop:\n",
    "        sino.append(\"True\")\n",
    "    elif palabra.isalpha() == False: #si no es alfabetino\n",
    "        sino.append(\"True\")\n",
    "    else:\n",
    "        sino.append(\"False\")\n",
    "orden[\"stop_espanol\"] = sino#añadimos la nueva columna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se clasifican las palabras en stopwords o no, para ahora crear la nueva tabla e isertarla en la base de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataFrame   = pd.DataFrame(data=orden)           \n",
    "\n",
    "tableName = \"tokenizado_linkedin_especialidades\"\n",
    "\n",
    "sqlEngine       = create_engine('mysql+pymysql://root:@127.0.0.1/TFG', pool_recycle=3600)\n",
    "\n",
    "dbConnection    = sqlEngine.connect()\n",
    "\n",
    " \n",
    "\n",
    "try:\n",
    "\n",
    "    frame           = dataFrame.to_sql(tableName, dbConnection, if_exists='fail');\n",
    "\n",
    "except ValueError as vx:\n",
    "\n",
    "    print(vx)\n",
    "\n",
    "except Exception as ex:   \n",
    "\n",
    "    print(ex)\n",
    "\n",
    "else:\n",
    "\n",
    "    print(\"Table %s created successfully.\"%tableName);   \n",
    "\n",
    "finally:\n",
    "\n",
    "    dbConnection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Con este código se tokeniza todos los textos, cambiando la tabla y la columna\n",
    "\n",
    "# Tiene una duración de 4 minutos para 1,2M de registros pero la máquina no soporta crear una tabla de 29M de registros"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b46891482e43448fca434b48535568d1fb045c476689b999f0b2c31bd471c5b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
